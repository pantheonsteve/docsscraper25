{% extends 'dashboard/base.html' %}

{% block title %}Management Commands Reference - DocAnalyzer{% endblock %}

{% block content %}
<div class="page-header">
    <h1>Management Commands Reference</h1>
    <p class="text-muted">
        Cheatsheet for all available <code>manage.py</code> commands used to operate the DocAnalyzer
        crawler and analytics.
    </p>
</div>

<style>
    .cmd-section {
        margin-bottom: 2rem;
        padding: 1.5rem;
        border-radius: 12px;
        background: #ffffff;
        box-shadow: 0 4px 16px rgba(0,0,0,0.05);
    }
    .cmd-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 0.75rem;
    }
    .cmd-name {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        font-weight: 600;
        font-size: 1rem;
    }
    .cmd-badge {
        display: inline-block;
        padding: 0.2rem 0.6rem;
        border-radius: 999px;
        font-size: 0.75rem;
        font-weight: 600;
        background: #eef2ff;
        color: #4338ca;
        text-transform: uppercase;
        letter-spacing: 0.04em;
    }
    .cmd-description {
        margin-bottom: 0.75rem;
        color: #4b5563;
    }
    pre.code-block {
        background: #0f172a;
        color: #e5e7eb;
        padding: 0.75rem 1rem;
        border-radius: 8px;
        font-size: 0.9rem;
        overflow-x: auto;
        margin-bottom: 0.75rem;
    }
    .cmd-meta {
        font-size: 0.8rem;
        color: #6b7280;
        margin-top: 0.25rem;
    }
    .cmd-args {
        margin-top: 0.5rem;
        font-size: 0.9rem;
    }
    .cmd-args dt {
        font-weight: 600;
    }
    .cmd-args dd {
        margin: 0 0 0.5rem 0;
        color: #4b5563;
    }
    .tag-group {
        margin-top: 0.25rem;
    }
    .tag {
        display: inline-block;
        padding: 0.1rem 0.5rem;
        margin-right: 0.3rem;
        border-radius: 999px;
        font-size: 0.75rem;
        background: #f3f4f6;
        color: #4b5563;
    }
</style>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py crawl</span>
        <span class="cmd-badge">Crawling</span>
    </div>
    <p class="cmd-description">
        Start a new documentation crawl for a given URL and client. This is the CLI equivalent of the
        “Start New Crawl” form in the dashboard.
    </p>
    <pre class="code-block">
# Basic: crawl a site for the default client
python manage.py crawl --url https://docs.dynatrace.com/docs

# With explicit client and depth limit
python manage.py crawl \
    --url https://docs.datadoghq.com \
    --client "Acme Corp" \
    --depth 5

# Limit pages and enable raw HTML + screenshots (requires Playwright)
python manage.py crawl \
    --url https://docs.pantheon.io \
    --client "My Customer" \
    --depth 4 \
    --max-pages 200 \
    --capture-html \
    --screenshots \
    --async</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--url</code>: Starting URL to crawl (required).</dd>
        <dd><code>--client</code>: Client name or ID; created automatically if it doesn’t exist.</dd>
        <dd><code>--depth</code>: Maximum crawl depth (default 5).</dd>
        <dd><code>--domains</code>: Comma-separated list of allowed domains.</dd>
        <dd><code>--max-pages</code>: Hard limit on number of pages (useful for test runs).</dd>
        <dd><code>--capture-html</code>: Store full raw HTML for each page.</dd>
        <dd><code>--screenshots</code>: Capture full-page screenshots via Playwright (async Celery tasks).</dd>
        <dd><code>--async</code>: Run crawl via Celery instead of blocking the terminal.</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py crawl_status</span>
        <span class="cmd-badge">Monitoring</span>
    </div>
    <p class="cmd-description">
        Inspect the status, timings, and statistics for a given crawl job.
    </p>
    <pre class="code-block">
# Human-readable summary
python manage.py crawl_status --job 57

# JSON output for scripts / integrations
python manage.py crawl_status --job 57 --json</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job</code>: ID of the <code>CrawlJob</code> to inspect.</dd>
        <dd><code>--json</code>: Output a machine-readable JSON document.</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py cancel_crawl</span>
        <span class="cmd-badge">Operations</span>
    </div>
    <p class="cmd-description">
        Cancel a running or pending crawl and revoke its Celery task.
    </p>
    <pre class="code-block">
# Cancel a running job
python manage.py cancel_crawl --job 57

# Force cancel even if the job is already completed/failed
python manage.py cancel_crawl --job 57 --force</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job</code>: ID of the job to cancel.</dd>
        <dd><code>--force</code>: Override status checks and mark the job cancelled anyway.</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py export_crawl</span>
        <span class="cmd-badge">Exports</span>
    </div>
    <p class="cmd-description">
        Export all pages from a crawl as JSON or CSV for offline analysis, reporting, or ingestion
        into other systems.
    </p>
    <pre class="code-block">
# Export to JSON (stdout)
python manage.py export_crawl --job 57 --format json

# Export to JSON file including raw HTML
python manage.py export_crawl \
    --job 57 \
    --format json \
    --include-html \
    --output exports/dynatrace-job57.json

# Export a lightweight CSV summary
python manage.py export_crawl \
    --job 57 \
    --format csv \
    --output exports/dynatrace-job57.csv</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job</code>: ID of the job to export.</dd>
        <dd><code>--format</code>: <code>json</code> or <code>csv</code> (default <code>json</code>).</dd>
        <dd><code>--output</code>: Path to save the export (otherwise prints to stdout).</dd>
        <dd><code>--include-html</code>: Include full <code>raw_html</code> in the JSON/CSV.</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py deduplicate_pages</span>
        <span class="cmd-badge">Cleanup</span>
    </div>
    <p class="cmd-description">
        Remove duplicate pages for a client, keeping the most recent copy of each URL. Useful
        after many experimental crawls.
    </p>
    <pre class="code-block">
# Dry-run: see what would be deleted without changing data
python manage.py deduplicate_pages --client-slug dynatrace --dry-run

# Actually deduplicate pages for a single client
python manage.py deduplicate_pages --client-slug dynatrace

# Deduplicate pages for all active clients
python manage.py deduplicate_pages --all-clients</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--client-slug</code>: Slug of the client whose pages you want to dedupe.</dd>
        <dd><code>--all-clients</code>: Run deduplication across all active clients.</dd>
        <dd><code>--dry-run</code>: Show what would be deleted without actually deleting.</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py reanalyze_pages</span>
        <span class="cmd-badge">Backfill / Analysis</span>
    </div>
    <p class="cmd-description">
        Re-run structural analysis (code blocks, learning objectives, content-type metrics)
        on already-crawled pages using the latest parser logic.
    </p>
    <pre class="code-block">
# Re-analyze all pages for a specific job
python manage.py reanalyze_pages --job-id 57

# Re-analyze a single page by ID
python manage.py reanalyze_pages --page-id 21115

# Re-analyze all pages for a client (by ID)
python manage.py reanalyze_pages --client-id 3

# Limit to the first 100 pages (for quick tests)
python manage.py reanalyze_pages --job-id 57 --limit 100</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--page-id</code>: Re-analyze one specific <code>CrawledPage</code> by ID.</dd>
        <dd><code>--job-id</code>: Re-analyze all pages from a specific <code>CrawlJob</code>.</dd>
        <dd><code>--client-id</code>: Re-analyze all pages for a given client.</dd>
        <dd><code>--limit</code>: Cap the number of pages processed (useful for experiments).</dd>
    </dl>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py analyze_content</span>
        <span class="cmd-badge">AI / Content Analysis</span>
    </div>
    <p class="cmd-description">
        Analyze crawled pages with AI to extract comprehensive metadata for lesson grouping and taxonomy building. 
        Uses a hybrid approach with spaCy (local NLP) + GPT-4o-mini to extract: hierarchical topics, 
        learning objectives with Bloom's taxonomy, prerequisite chains, page summaries, audience levels, 
        key concepts, quality indicators, and auto-generates embeddings for learning objectives. 
        Cost: ~$0.00015/page for analysis + ~$0.00003/page for LO embeddings.
    </p>
    <pre class="code-block">
# Analyze all pages in a job
python manage.py analyze_content --job-id 57

# Test with first 10 pages (recommended before full analysis)
python manage.py analyze_content --job-id 57 --limit 10

# Analyze a single page
python manage.py analyze_content --page-id 21115

# Analyze all pages for a client
python manage.py analyze_content --client-id 3

# Force re-analysis (overwrite existing AI analysis)
python manage.py analyze_content --job-id 57 --force

# Dry run to estimate costs without processing
python manage.py analyze_content --job-id 57 --dry-run</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job-id</code>: Analyze all pages in a specific crawl job.</dd>
        <dd><code>--page-id</code>: Analyze a single page by ID.</dd>
        <dd><code>--client-id</code>: Analyze all pages belonging to a client.</dd>
        <dd><code>--limit</code>: Process only the first N pages (useful for testing).</dd>
        <dd><code>--force</code>: Re-analyze pages that already have AI analysis.</dd>
        <dd><code>--dry-run</code>: Show what would be analyzed and estimate costs.</dd>
    </dl>
    <div class="tag-group">
        <span class="tag">GPT-4o-mini</span>
        <span class="tag">spaCy NLP</span>
        <span class="tag">Hierarchical Topics</span>
        <span class="tag">Bloom Taxonomy</span>
        <span class="tag">Learning Objectives</span>
        <span class="tag">LO Embeddings</span>
        <span class="tag">Prerequisites</span>
        <span class="tag">Key Concepts</span>
        <span class="tag">Quality Indicators</span>
        <span class="tag">Diátaxis Framework</span>
        <span class="tag">~$0.00018/page</span>
    </div>
    <p class="cmd-meta">
        <strong>What's extracted:</strong>
    </p>
    <ul class="cmd-meta" style="margin-left: 1.5rem; margin-top: 0.5rem; color: #4b5563; font-size: 0.9rem;">
        <li><strong>Topics</strong>: 3-7 hierarchical topics with parent-child relationships, relevance scores, and categories</li>
        <li><strong>Learning Objectives</strong>: Action-oriented objectives with Bloom's taxonomy levels (remember→create), difficulty, time estimates, measurability</li>
        <li><strong>Learning Objective Embeddings</strong>: Vector embeddings for each LO to enable semantic clustering and lesson grouping</li>
        <li><strong>Prerequisites</strong>: Dependency chain with type (knowledge/skill/tool), importance (essential/recommended), descriptions</li>
        <li><strong>Key Concepts</strong>: Technical terms with definitions, marked as new (introduced here) or assumed (prerequisite knowledge)</li>
        <li><strong>Page Summary</strong>: 1-2 sentence summary for efficient clustering</li>
        <li><strong>Audience Level</strong>: Beginner, intermediate, or advanced classification</li>
        <li><strong>Doc Type</strong>: Re-classified using Diátaxis framework (tutorial, how-to, reference, concept, etc.)</li>
        <li><strong>Quality Indicators</strong>: Completeness score, improvement suggestions, missing elements (code examples, visuals, troubleshooting)</li>
        <li><strong>Related Topics</strong>: Cross-linking topics for building documentation graphs</li>
    </ul>
    <p class="cmd-meta" style="margin-top: 1rem;">
        <strong>Requirements:</strong> OPENAI_API_KEY in .env. First run downloads spaCy model (en_core_web_sm) automatically.
    </p>
    <p class="cmd-meta">
        <strong>Cost optimization:</strong> Automatically skips pages with doc_type: navigation, landing, changelog (these are typically non-content pages). 
        Pages with doc_type 'unknown' ARE analyzed because the AI reclassifies them using the Diátaxis framework.
        Use <code>--limit 10</code> to test on a small sample before running full analysis.
    </p>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py generate_embeddings</span>
        <span class="cmd-badge">AI / Embeddings</span>
    </div>
    <p class="cmd-description">
        Generate OpenAI embeddings (text-embedding-3-small) for crawled pages. Creates vector
        representations for full pages and individual sections, enabling semantic search and
        RAG applications.
    </p>
    <pre class="code-block">
# Generate embeddings for all pages in a job
python manage.py generate_embeddings --job-id 57

# Generate embeddings for a single page
python manage.py generate_embeddings --page-id 21115

# Generate embeddings for all pages of a client
python manage.py generate_embeddings --client-id 3

# Force re-generation (overwrite existing embeddings)
python manage.py generate_embeddings --job-id 57 --force

# Limit to first N pages (for testing costs)
python manage.py generate_embeddings --job-id 57 --limit 100</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job-id</code>: Generate embeddings for all pages in a specific crawl job.</dd>
        <dd><code>--page-id</code>: Generate embeddings for a single page by ID.</dd>
        <dd><code>--client-id</code>: Generate embeddings for all pages belonging to a client.</dd>
        <dd><code>--limit</code>: Maximum number of pages to process (useful for cost control).</dd>
        <dd><code>--force</code>: Recompute embeddings even if they already exist.</dd>
    </dl>
    <div class="cmd-meta">
        <strong>Cost:</strong> ~$0.00005 per page (~1 cent per 200 pages) using text-embedding-3-small.<br>
        <strong>Requirements:</strong> <code>OPENAI_API_KEY</code> must be set in <code>.env</code>.<br>
        <strong>Output:</strong> Creates <code>page_embedding</code> (full page vector) and <code>section_embeddings</code> (per-section vectors).
    </div>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py build_taxonomy</span>
        <span class="cmd-badge">AI / Taxonomy</span>
    </div>
    <p class="cmd-description">
        Build a hierarchical documentation taxonomy from AI-analyzed pages using semantic clustering 
        and GPT-4o-mini. Creates organized topic categories with rich metadata, learning outcomes, 
        and prerequisite relationships. Outputs JSON, Markdown, and prerequisite graph visualizations.
    </p>
    <pre class="code-block">
# Build taxonomy with optimal settings (recommended)
python manage.py build_taxonomy \
    --client-id 5 \
    --n-clusters 50 \
    --min-cluster-size 5 \
    --max-cluster-size 30

# Quick test with first 100 pages
python manage.py build_taxonomy \
    --client-id 5 \
    --n-clusters 20 \
    --limit 100

# Skip GPT summaries (faster, but less useful)
python manage.py build_taxonomy \
    --client-id 5 \
    --skip-summaries

# Use different embedding field for clustering
python manage.py build_taxonomy \
    --client-id 5 \
    --embedding-type page

# Custom output directory
python manage.py build_taxonomy \
    --client-id 5 \
    --output-dir ./custom_taxonomies/

# Dry run to see what would be generated
python manage.py build_taxonomy \
    --client-id 5 \
    --dry-run</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--client-id</code>: Client ID to build taxonomy for (required).</dd>
        <dd><code>--n-clusters</code>: Number of clusters to create. Use 'auto' for automatic selection or specific number (default: auto).</dd>
        <dd><code>--min-cluster-size</code>: Minimum pages per cluster (default: 5).</dd>
        <dd><code>--max-cluster-size</code>: Maximum pages per cluster (default: 50).</dd>
        <dd><code>--clustering-method</code>: Algorithm to use: 'kmeans', 'hierarchical', or 'dbscan' (default: kmeans).</dd>
        <dd><code>--embedding-type</code>: Which embeddings to use: 'lo' (learning objectives), 'page', or 'section' (default: lo).</dd>
        <dd><code>--limit</code>: Only process first N pages (useful for testing).</dd>
        <dd><code>--skip-summaries</code>: Skip GPT-generated cluster summaries (faster but less useful).</dd>
        <dd><code>--output-dir</code>: Directory to save outputs (default: ./taxonomies/).</dd>
        <dd><code>--dry-run</code>: Show what would be generated without creating files.</dd>
    </dl>
    <div class="tag-group">
        <span class="tag">GPT-4o-mini</span>
        <span class="tag">Hierarchical Topics</span>
        <span class="tag">Semantic Clustering</span>
        <span class="tag">Learning Outcomes</span>
        <span class="tag">Prerequisites</span>
        <span class="tag">~$0.02/taxonomy</span>
    </div>
    <p class="cmd-meta">
        <strong>What's generated:</strong>
    </p>
    <ul class="cmd-meta" style="margin-left: 1.5rem; margin-top: 0.5rem; color: #4b5563; font-size: 0.9rem;">
        <li><strong>Hierarchical Topics</strong>: 10-20 parent categories with rich metadata (overview, learning outcomes, target audience, key technologies)</li>
        <li><strong>Module Clusters</strong>: 20-100 semantically grouped modules with AI-generated names and descriptions</li>
        <li><strong>Statistics</strong>: Difficulty breakdowns, time estimates, content type analysis, cohesion scores</li>
        <li><strong>Prerequisite Graph</strong>: Directed graph showing learning dependencies between pages and concepts</li>
        <li><strong>Export Formats</strong>: JSON (full data), Markdown (readable), Mermaid (graph visualization), DOT (graph visualization)</li>
    </ul>
    <p class="cmd-meta" style="margin-top: 1rem;">
        <strong>Requirements:</strong> Pages must be AI-analyzed first (run <code>analyze_content</code>). 
        OPENAI_API_KEY required for rich summaries.
    </p>
    <p class="cmd-meta">
        <strong>Output files:</strong> {client_slug}_taxonomy_{date}.json, {client_slug}_taxonomy_{date}.md, 
        {client_slug}_prerequisite_graph.mmd, {client_slug}_prerequisite_graph.dot, {client_slug}_taxonomy_report.txt
    </p>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py sync_doc_types</span>
        <span class="cmd-badge">Cleanup / Sync</span>
    </div>
    <p class="cmd-description">
        Sync the <code>doc_type</code> field with <code>ai_doc_type</code> for pages that have been 
        AI-analyzed. Useful after running content analysis to update the primary doc_type classification 
        with the AI's more accurate Diátaxis-based classification.
    </p>
    <pre class="code-block">
# Sync all pages (dry-run first)
python manage.py sync_doc_types --dry-run

# Actually sync all pages
python manage.py sync_doc_types

# Sync only pages in a specific job
python manage.py sync_doc_types --job-id 57

# Sync only pages for a specific client
python manage.py sync_doc_types --client-id 3</pre>
    <dl class="cmd-args">
        <dt>Key options</dt>
        <dd><code>--job-id</code>: Sync only pages in a specific crawl job.</dd>
        <dd><code>--client-id</code>: Sync only pages for a specific client.</dd>
        <dd><code>--dry-run</code>: Show what would be updated without making changes.</dd>
    </dl>
    <p class="cmd-meta">
        <strong>When to use:</strong> After running <code>analyze_content</code> to update the primary 
        doc_type field with AI classifications. This helps keep data consistent across the system.
    </p>
</div>

<div class="cmd-section">
    <div class="cmd-header">
        <span class="cmd-name">python manage.py start_celery</span>
        <span class="cmd-badge">Infrastructure</span>
    </div>
    <p class="cmd-description">
        Convenience command to start the Celery worker (and optionally Celery beat) for local
        development using the correct queues and project settings.
    </p>
    <pre class="code-block">
# Start Celery worker (required for crawls, screenshots, embeddings)
python manage.py start_celery

# Start Celery worker + beat (for scheduled tasks)
python manage.py start_celery --beat</pre>
    <dl class="cmd-args">
        <dt>What it does</dt>
        <dd>Starts <code>celery -A config worker -l info -Q celery,crawling,analysis</code> using the same Python/venv as <code>manage.py</code>.</dd>
        <dd>With <code>--beat</code>, also starts <code>celery -A config beat -l info</code> in the same terminal.</dd>
        <dd>Keeps both processes running until you stop the command with <code>Ctrl+C</code>.</dd>
    </dl>
</div>

{% endblock %}


